{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed libraries\n",
    "import timeit\n",
    "tic=timeit.default_timer()\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import implicit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "import pandas.io.sql as sql\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "\n",
    "#Connect to Snowflake database and load data\n",
    "cnn = snowflake.connector.connect(\n",
    "                user='bagabi',\n",
    "                password='Ben@2020',\n",
    "                account='sears_hs_prod.us-east-1'\n",
    "                )\n",
    "cursor = cnn.cursor()\n",
    "\n",
    "generic = \"\"\"SELECT \n",
    " a.PRT_ORD_DT,\n",
    " a.PRT_ORD_LN_NO,\n",
    " a.PRT_DIV_NO,\n",
    " a.PRT_PRC_LIS_SRC_NO,\n",
    " a.PRT_NO,\n",
    " a.div_pls,\n",
    " a.item_id, \n",
    " b.ATTRIBUTEID,  \n",
    " b.PARTID\n",
    " FROM\n",
    "(\n",
    "SELECT PRT_ORD_DT,\n",
    " PRT_ORD_LN_NO,\n",
    " PRT_DIV_NO,\n",
    " PRT_PRC_LIS_SRC_NO,\n",
    " PRT_NO, \n",
    " LPAD(prt_div_no, 4, '0') || LPAD(prt_prc_lis_src_no, 3, '0') AS div_pls,\n",
    " LPAD(prt_div_no, 4, '0') || LPAD(prt_prc_lis_src_no, 3, '0') || prt_no AS item_id\n",
    "FROM\n",
    "\"PRD_PARTSDIRECT\".\"BATCH\".\"PRTDTPL_PRT_LN\"\n",
    ") a\n",
    " left join \"PRD_KM\".\"KM\".\"PART_ATTRIBUTE\" b\n",
    " ON b.PARTID=a.item_id\n",
    " WHERE b.ATTRIBUTEID=138 \n",
    " AND b.ACTIVE = 1\n",
    " AND a.prt_ord_dt >= '2019-01-01' \n",
    "\"\"\"\n",
    "query_generic = pd.read_sql_query(generic, cnn)\n",
    "\n",
    "#Generic parts catalog recommendations\n",
    "data = query_generic[['DIV_PLS','ITEM_ID','PRT_ORD_LN_NO']]\n",
    "data = data.groupby(['DIV_PLS','ITEM_ID']).agg(\n",
    "    Count=pd.NamedAgg(column='PRT_ORD_LN_NO', aggfunc='nunique')   \n",
    ")\n",
    "data.reset_index(drop=False, inplace=True)\n",
    "\n",
    "user_category = CategoricalDtype(sorted(data.DIV_PLS.unique()), ordered=True)\n",
    "item_category = CategoricalDtype(sorted(data.ITEM_ID.unique()), ordered=True)\n",
    "\n",
    "row = data['DIV_PLS'].astype(user_category).cat.codes\n",
    "col = data['ITEM_ID'].astype(item_category).cat.codes\n",
    "\n",
    "#Transform two column raw data frame into item-item sparse matrix\n",
    "sparse_matrix = csr_matrix((data['Count'], (row, col)),shape=(user_category.categories.size,item_category.categories.size))\n",
    "sparse_df = pd.SparseDataFrame(sparse_matrix,index=user_category.categories,columns=item_category.categories,default_fill_value=0)\n",
    "\n",
    "#Using sparse matrix create co-occurrence matrix\n",
    "co_matrix = sparse_matrix.transpose().dot(sparse_matrix)\n",
    "co_matrix.setdiag(0)\n",
    "co_df = pd.SparseDataFrame(co_matrix,\n",
    "                               index=item_category.categories,\n",
    "                               columns=item_category.categories,\n",
    "                               default_fill_value=0)\n",
    "\n",
    "#Filter out nonzero index in co-occurrence matrix\n",
    "idx = pd.np.nonzero(co_matrix)\n",
    "\n",
    "#Create subjects and peers list\n",
    "rows = idx[0]\n",
    "columns = idx[1]\n",
    "subjects = [item_category.categories[i] for i in rows]\n",
    "peers = [item_category.categories[i] for i in columns]\n",
    "\n",
    "#Extract the co-occurrence counts for pair of parts\n",
    "occur = co_matrix[idx].tolist()[0]\n",
    "\n",
    "#Create final recommended dataframe\n",
    "df_recommend = pd.DataFrame.from_records(zip(subjects, peers, occur),\n",
    "                                         columns = ['PARTID', 'RECOMMENDED_PARTID', 'PCT_SCORE'])\n",
    "result = df_recommend.groupby(['PARTID','RECOMMENDED_PARTID']).agg({'PCT_SCORE': 'sum'})\n",
    "result = result.groupby(level=['PARTID']).apply(lambda x: 100 * x / float(x.sum()))\n",
    "result.reset_index(drop=False,inplace=True)\n",
    "result['PCT_SCORE'] = round(result['PCT_SCORE']/1,1)\n",
    "finalResults = result.sort_values(by=['PARTID','PCT_SCORE'], ascending=[False,False])\n",
    "\n",
    "finalResults = finalResults[(finalResults['PARTID'] != finalResults['PARTID'].shift()) | (finalResults['RECOMMENDED_PARTID'] \\\n",
    "                                                                         != finalResults['RECOMMENDED_PARTID'].shift())]\n",
    "\n",
    "finalResults['PARTID'] = finalResults['PARTID'].astype('str')\n",
    "finalResults['RECOMMENDED_PARTID'] = finalResults['RECOMMENDED_PARTID'].astype('str')                                         \n",
    "#finalResults.to_csv('PART_RECOMMENDATION.csv', index=False)\n",
    "\n",
    "toc=timeit.default_timer()\n",
    "Elapsed = toc - tic \n",
    "hours, rem = divmod(Elapsed, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
